{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3- Features Selection\n",
    "Name: Vivian Lim Bi Fang \n",
    "\n",
    "ID: P101458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google_Translate</th>\n",
       "      <th>Clean_Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Service is very friendly staff made ??us very ...</td>\n",
       "      <td>Service    friendly    staff    made    us  ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Our family stayed one night and found this hot...</td>\n",
       "      <td>family    stayed    one    night    found   ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mid-sized room but clean. Good breakfast food ...</td>\n",
       "      <td>Midsized    room    clean    Good    breakfa...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Hotel is on the top of a shopping Pavillion, L...</td>\n",
       "      <td>Hotel    top    shopping    Pavillion    Lot...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I am comfortable with accommodation in penang ...</td>\n",
       "      <td>comfortable    accommodation    penang    ha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Google_Translate  \\\n",
       "0  Service is very friendly staff made ??us very ...   \n",
       "1  Our family stayed one night and found this hot...   \n",
       "2  Mid-sized room but clean. Good breakfast food ...   \n",
       "3  Hotel is on the top of a shopping Pavillion, L...   \n",
       "4  I am comfortable with accommodation in penang ...   \n",
       "\n",
       "                                          Clean_Text Sentiment  \n",
       "0    Service    friendly    staff    made    us  ...  Positive  \n",
       "1    family    stayed    one    night    found   ...  Positive  \n",
       "2    Midsized    room    clean    Good    breakfa...  Positive  \n",
       "3    Hotel    top    shopping    Pavillion    Lot...  Positive  \n",
       "4    comfortable    accommodation    penang    ha...  Positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data \n",
    "import pandas as pd\n",
    "\n",
    "# hotel_reviews_data is being preprocess by removing stopwords, punctuation and numbers in assignment 2.\n",
    "hotel_reviews_data = pd.read_csv(\"Labelled_Hotel_Review_Data.csv\")\n",
    "hotel_reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemmatized</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Service friendly staff make us comfortable fam...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>family stay one night find hotel much satisfac...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Midsized room clean Good breakfast food option...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Hotel top shop Pavillion Lot suitable couple w...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>comfortable accommodation penang hard rock com...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>park access card not use due fluctuations offi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>Cubicles kitchens excessive stink not directly...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Necessary provide sufficient area train layout...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>everything ok except small rat chamber</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>Small room light blanket theres lot room nyamu...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Lemmatized Sentiment\n",
       "0   Service friendly staff make us comfortable fam...  Positive\n",
       "1   family stay one night find hotel much satisfac...  Positive\n",
       "2   Midsized room clean Good breakfast food option...  Positive\n",
       "3   Hotel top shop Pavillion Lot suitable couple w...  Positive\n",
       "4   comfortable accommodation penang hard rock com...  Positive\n",
       "..                                                ...       ...\n",
       "93  park access card not use due fluctuations offi...  Negative\n",
       "94  Cubicles kitchens excessive stink not directly...  Negative\n",
       "95  Necessary provide sufficient area train layout...  Negative\n",
       "96             everything ok except small rat chamber  Negative\n",
       "97  Small room light blanket theres lot room nyamu...  Negative\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization \n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def lemm(sentences):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordlist = word_tokenize(sentences)\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    for word in wordlist:\n",
    "        output = lemmatizer.lemmatize(word, pos= \"v\")\n",
    "        result.append(output)\n",
    "       \n",
    "    result =  \" \".join(result)\n",
    "    return result\n",
    "\n",
    "hotel_reviews_data[\"Lemmatized\"] = hotel_reviews_data.apply(lambda row : lemm(row[\"Clean_Text\"]), axis=1)\n",
    "hotel_reviews_data = hotel_reviews_data[['Lemmatized','Sentiment']]\n",
    "hotel_reviews_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CSV file for later use in assignment 4\n",
    "\n",
    "hotel_reviews_data.to_csv(r'C:\\Users\\vivia\\Documents\\Data Science\\Unstructure Data Analysis\\Assignments & Project\\assignment4.csv', header= True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for data balanced to avoid bias \n",
    "# dataset is balanced. Not necessary to conduct bookstrapping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sentiment_count = hotel_reviews_data[\"Sentiment\"].value_counts()\n",
    "# print (sentiment_count)\n",
    "# plt.figure(figsize=(6,4))\n",
    "# sns.barplot(sentiment_count.index, sentiment_count.values, alpha =0.5, )\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Sentiment',data=hotel_reviews_data,palette='rainbow')\n",
    "plt.ylabel (\"Frequency\")\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "20\n",
      "78\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test set \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_data = hotel_reviews_data[\"Lemmatized\"].values\n",
    "label_data = hotel_reviews_data[\"Sentiment\"].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_data, label_data, test_size = 0.2, random_state=42)\n",
    "print (len(x_train))\n",
    "print (len(x_test))\n",
    "print (len(y_train))\n",
    "print (len(y_test))\n",
    "\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735\n",
      "735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW = CountVectorizer()\n",
    "BOW.fit(x_train)\n",
    "BOW_train = BOW.transform(x_train)\n",
    "# print (BOW_train)\n",
    "BOW_test = BOW.transform(x_test)\n",
    "\n",
    "print ((BOW_train.shape[1]))\n",
    "print ((BOW_test.shape[1]))\n",
    "\n",
    "# BOW_train\n",
    "# print(BOW_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access</th>\n",
       "      <th>accessible</th>\n",
       "      <th>accommodation</th>\n",
       "      <th>accommodations</th>\n",
       "      <th>accordance</th>\n",
       "      <th>according</th>\n",
       "      <th>active</th>\n",
       "      <th>activities</th>\n",
       "      <th>add</th>\n",
       "      <th>addition</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>workers</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>worthwhile</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>yg</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 735 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   access  accessible  accommodation  accommodations  accordance  according  \\\n",
       "0       0           0              0               0           0          0   \n",
       "1       0           0              1               0           0          0   \n",
       "2       0           0              0               0           0          0   \n",
       "3       0           0              0               0           0          0   \n",
       "4       0           0              0               0           0          0   \n",
       "\n",
       "   active  activities  add  addition  ...  work  workers  world  worth  \\\n",
       "0       0           0    0         0  ...     0        0      0      0   \n",
       "1       0           0    0         0  ...     0        0      0      0   \n",
       "2       0           0    0         0  ...     0        0      0      0   \n",
       "3       0           0    0         0  ...     0        0      0      0   \n",
       "4       0           0    0         0  ...     0        0      0      0   \n",
       "\n",
       "   worthwhile  would  year  yet  yg  yummy  \n",
       "0           0      0     0    0   0      0  \n",
       "1           0      0     0    0   0      0  \n",
       "2           0      0     0    0   0      0  \n",
       "3           0      0     0    0   0      0  \n",
       "4           0      0     0    0   0      0  \n",
       "\n",
       "[5 rows x 735 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_BOW = pd.DataFrame(BOW_train.toarray(), columns=BOW.get_feature_names())\n",
    "word_BOW.head()\n",
    "\n",
    "\n",
    "# print(word_BOW[0:10])\n",
    "# print(BOW_train[0:2].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on NB \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_BOW = MultinomialNB()\n",
    "NB_BOW.fit(BOW_train, y_train)\n",
    "predict_NB_BOW = NB_BOW.predict(BOW_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      1.00      0.93         7\n",
      "    Positive       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.94      0.96      0.95        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test for F1 score, precision and recall \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "result_BOW = classification_report(predict_NB_BOW, y_test)\n",
    "print (result_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the accuracy of TF-IDF using unigram and compare with ngram range from 1 to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def TFIDF_ngram (n_gram, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    TFIDF = TfidfVectorizer(analyzer = \"word\", ngram_range = n_gram)\n",
    "    TFIDF.fit (x_train)\n",
    "    TFIDF_train = TFIDF.transform (x_train)\n",
    "    TFIDF_test = TFIDF.transform (x_test)\n",
    "#     word_TFIDF = pd.DataFrame(TFIDF_train.toarray(), columns = TFIDF.get_feature_names())\n",
    "#     word_TFIDF\n",
    "    \n",
    "#     print (TFIDF_train)\n",
    "    NB_TFIDF = MultinomialNB()\n",
    "    NB_TFIDF.fit(TFIDF_train, y_train)\n",
    "    predict_NB_TFIDF = NB_TFIDF.predict(TFIDF_test)\n",
    "#     print (predict_NB_TFIDF)\n",
    "    \n",
    "    print (\"\")\n",
    "    print (\"ngram =\" +str(n))\n",
    "\n",
    "    result_TFIDF = classification_report(predict_NB_TFIDF, y_test)\n",
    "    print (result_TFIDF)\n",
    "\n",
    "    return (result_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ngram =(1, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      1.00      0.93         7\n",
      "    Positive       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.94      0.96      0.95        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n",
      "\n",
      "ngram =(2, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.56      0.59         9\n",
      "    Positive       0.67      0.73      0.70        11\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.65      0.64      0.64        20\n",
      "weighted avg       0.65      0.65      0.65        20\n",
      "\n",
      "\n",
      "ngram =(3, 3)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.44      0.62        18\n",
      "    Positive       0.17      1.00      0.29         2\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.58      0.72      0.45        20\n",
      "weighted avg       0.92      0.50      0.58        20\n",
      "\n",
      "\n",
      "ngram =(4, 4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.40      0.57        20\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.40        20\n",
      "   macro avg       0.50      0.20      0.29        20\n",
      "weighted avg       1.00      0.40      0.57        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ngram = [(1,1), (2,2), (3,3), (4,4)]\n",
    "# ngram = [(1,1), (1,2), (1,3), (1,4)]\n",
    "for n in ngram:\n",
    "    TFIDF_ngram (n, x_train, y_train, x_test, y_test)\n",
    "#     print (n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclude that, direct comparison of unigram to bigram, trigram is not encourage. This is because forming exactly bigram / trigram sequence increase sparsing thus reduce efficiecy. Besides, finding exactly the word sequece (x,x1) / (x,x2,x3) would be very hard which result the worst accuracy. \n",
    "\n",
    "Further approach can be tried using different ngram_range such as ngram = [(1,1), (1,2), (1,3), (1,4)]. Surprising, the result are the same thus, unigram is sufficient for this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding (Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new training and test set for tagged documents purpose\n",
    "\n",
    "Doc2vec_data = hotel_reviews_data[[\"Lemmatized\", \"Sentiment\"]]\n",
    "# print (Doc2vec_data.head())\n",
    "\n",
    "\n",
    "train, test = train_test_split(Doc2vec_data, test_size = 0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(sentence):\n",
    "    tokens = []\n",
    "#     print (sentence)\n",
    "    for sent in nltk.sent_tokenize(sentence):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "#             print (word)\n",
    "#             print (len(word))\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# tagged the documents with unique ID\n",
    "train_tagged = train.apply(lambda x : TaggedDocument(words = tokenize_text(x[\"Lemmatized\"]),tags=[x.Sentiment]), axis=1)\n",
    "# print (train_tagged.values)\n",
    "test_tagged = test.apply(lambda x : TaggedDocument(words = tokenize_text(x[\"Lemmatized\"]),tags=[x.Sentiment]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative =5, hs=0, min_count =1, sample=0, workers = 8, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "for epoch in range (50):\n",
    "    model_dbow.train(utils.shuffle([x for x in train_tagged.values]), total_examples = len(train_tagged.values), epochs =1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final vector feature\n",
    "# infer_vector to create document vectors for another sample corpus\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "#     print (targets, regressors)\n",
    "    return targets, regressors\n",
    "\n",
    "Doc2Vec_y_train, Doc2Vec_x_train = vec_for_learning(model_dbow, train_tagged)\n",
    "# print (Doc2Vec_x_train)\n",
    "Doc2Vec_y_test, Doc2Vec_x_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.78      0.78        40\n",
      "    Positive       0.77      0.79      0.78        38\n",
      "\n",
      "    accuracy                           0.78        78\n",
      "   macro avg       0.78      0.78      0.78        78\n",
      "weighted avg       0.78      0.78      0.78        78\n",
      "\n",
      "Testing set prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.60      0.67        10\n",
      "    Positive       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.71      0.70      0.70        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  run in classifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB #dealing with real-values features so use Gaussian NB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "NB_Doc2Vec = GaussianNB()\n",
    "NB_Doc2Vec.fit(Doc2Vec_x_train, Doc2Vec_y_train)\n",
    "predict_NB_Doc2Vec_train = NB_Doc2Vec.predict(Doc2Vec_x_train)\n",
    "predict_NB_Doc2Vec_test = NB_Doc2Vec.predict(Doc2Vec_x_test)\n",
    "\n",
    "\n",
    "# do this step for model parameter tuning\n",
    "result_Doc2Vec_train = classification_report(predict_NB_Doc2Vec_train, Doc2Vec_y_train)\n",
    "print(\"Training set prediction:\")\n",
    "print (result_Doc2Vec_train)\n",
    "\n",
    "result_Doc2Vec_test = classification_report(predict_NB_Doc2Vec_test, Doc2Vec_y_test)\n",
    "print(\"Testing set prediction:\")\n",
    "print (result_Doc2Vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From the classification reports (precision, recall, f1-score) shown above, \n",
    "\n",
    "1) TF-IDF (unigram) & BOW has the same accuracy which is 0.96, 0.95, 0.95 respectively.\n",
    "\n",
    "2) Doc2Vec has lower accuracy (0.74, 0.65, 0.65) which might due to presence of noises or parameter tuning issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
